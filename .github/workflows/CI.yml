name: CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  BUILD_TYPE: Debug

jobs:
  build:
    strategy:
      matrix:
        os: [ubuntu-24.04, macos-13, macos-14]
        mpi: [MPICH, OpenMPI]

    runs-on: ${{matrix.os}}

    steps:
    - uses: actions/checkout@v4

    - name: Install ${{matrix.mpi}}
      run: |
        case $RUNNER_OS in
        Linux)
            case "${{matrix.mpi}}" in
                MPICH) packages=libmpich-dev;;
                OpenMPI) packages=libopenmpi-dev;;
            esac
            sudo apt-get update
            sudo apt-get install $packages
            ;;
        macOS)
            # The Homebrew MPICH doesn't have the `mpi-f08` Fortran
            # module, but cmake insists on it. We thus use MacPorts
            # instead.

            # case "${{matrix.mpi}}" in
            #     MPICH) packages=mpich;;
            #     OpenMPI) packages=open-mpi;;
            # esac
            # brew install $packages

            # Install MacPorts
            case "${{matrix.os}}" in
                macos-13)
                    wget https://github.com/macports/macports-base/releases/download/v2.10.1/MacPorts-2.10.1-13-Ventura.pkg
                    sudo /usr/sbin/installer -pkg MacPorts-2.10.1-13-Ventura.pkg -target /
                    rm MacPorts-2.10.1-13-Ventura.pkg
                    ;;
                macos-14)
                    wget https://github.com/macports/macports-base/releases/download/v2.10.1/MacPorts-2.10.1-14-Sonoma.pkg
                    sudo /usr/sbin/installer -pkg MacPorts-2.10.1-14-Sonoma.pkg -target /
                    rm MacPorts-2.10.1-14-Sonoma.pkg
                    ;;
            esac

            echo /opt/local/bin >> $GITHUB_PATH
            echo /opt/local/sbin >> $GITHUB_PATH
            export "PATH=/opt/local/bin:/opt/local/sbin:$PATH"
            sudo port sync

            case "${{matrix.mpi}}" in
                MPICH) packages='mpich-default +gcc14';;
                OpenMPI) packages='openmpi-default +gcc14';;
            esac
            sudo port install $packages
            ;;
        esac

    - name: Configure
      run: |
        case $RUNNER_OS in
        Linux)
            cmake -B ${{github.workspace}}/build \
                -DCMAKE_C_COMPILER=mpicc \
                -DCMAKE_CXX_COMPILER=mpicxx \
                -DCMAKE_Fortran_COMPILER=mpif90 \
                -DCMAKE_BUILD_TYPE=${{env.BUILD_TYPE}} \
                -DCMAKE_INSTALL_PREFIX=$HOME/mpiwrapper-${{matrix.mpi}}
            ;;
        macOS)
            case ${{matrix.mpi}} in
            MPICH)
                # We cannot use the compiler wrappers. These inject
                # `-flat_namespace` into the link flags. We cannot
                # have this; a plugin such as `libmpiwrapper.so` must
                # use `-twolevel_namespace`. Otherwise, the MPI calls
                # in `libmpiwrapper.so` would at run-time be resolved
                # to MPItrampoline instead of MPICH, leading to an
                # infinite recursion, stack overflow, and segfault.
                #
                # Even when using MPICH this way, the Fortran MPI
                # functions do not work. They are implemented by
                # calling the respective C MPI functions, which live
                # in a different shared library. MPICH's flat
                # namespace means that the Fortran MPI functions then
                # call the MPItrampoline C functions.
                #
                # Configuring MPICH with the option
                # `--enable-two-level-namespace` avoids this problem.
                # We currently don't do this because we would need to
                # build MPICH from source, which is a bit tedious to
                # implement here.
                cmake -B ${{github.workspace}}/build \
                    -DCMAKE_C_COMPILER=gcc-mp-14 \
                    -DCMAKE_CXX_COMPILER=g++-mp-14 \
                    -DCMAKE_Fortran_COMPILER=gfortran-mp-14 \
                    -DMPI_C_ADDITIONAL_INCLUDE_DIRS=/opt/local/include/mpich-mp \
                    -DMPI_C_LIB_NAMES='mpi;pmpi' \
                    -DMPI_CXX_ADDITIONAL_INCLUDE_DIRS=/opt/local/include/mpich-mp \
                    -DMPI_CXX_LIB_NAMES='mpicxx;mpi;pmpi' \
                    -DMPI_Fortran_ADDITIONAL_INCLUDE_DIRS=/opt/local/include/mpich-mp \
                    -DMPI_Fortran_LIB_NAMES='mpifort;mpi;pmpi' \
                    -DMPI_mpi_LIBRARY=/opt/local/lib/mpich-mp/libmpi.dylib \
                    -DMPI_mpicxx_LIBRARY=/opt/local/lib/mpich-mp/libmpicxx.dylib \
                    -DMPI_mpifort_LIBRARY=/opt/local/lib/mpich-mp/libmpifort.dylib \
                    -DMPI_pmpi_LIBRARY=/opt/local/lib/mpich-mp/libpmpi.dylib \
                    -DMPIEXEC_EXECUTABLE=/opt/local/bin/mpiexec-mpich-mp \
                    -DCMAKE_BUILD_TYPE=${{env.BUILD_TYPE}} \
                    -DCMAKE_INSTALL_PREFIX=$HOME/mpiwrapper-${{matrix.mpi}}
                ;;
            OpenMPI)
                # The compiler wrappers have non-standard names
                cmake -B ${{github.workspace}}/build \
                    -DCMAKE_C_COMPILER=mpicc-openmpi-mp \
                    -DCMAKE_CXX_COMPILER=mpicxx-openmpi-mp \
                    -DCMAKE_Fortran_COMPILER=mpifort-openmpi-mp \
                    -DCMAKE_EXE_LINKER_FLAGS=-ld_classic \
                    -DMPIEXEC_EXECUTABLE=/opt/local/bin/mpiexec-openmpi-mp \
                    -DCMAKE_BUILD_TYPE=${{env.BUILD_TYPE}} \
                    -DCMAKE_INSTALL_PREFIX=$HOME/mpiwrapper-${{matrix.mpi}}
                ;;
            esac
            ;;
        esac

    - name: Build
      run: cmake --build ${{github.workspace}}/build --verbose

    - name: Test
      working-directory: ${{github.workspace}}/build
      run: ctest -C ${{env.BUILD_TYPE}}

    - name: Install
      run: cmake --install ${{github.workspace}}/build --verbose
